{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"deployment/intro/","title":"intro","text":""},{"location":"deployment/intro/#how-to-deploy-models-trained-with-detectron2","title":"How to Deploy Models Trained with Detectron2","text":"<p>The repo comes with an unassuming script called export_model.py, it uses the rest of the package just as an API, and can be used as a standalone script or copied into your own repo (so that you don't have to clone detectron2).</p> <p>It is overly verbose, so I've rewritten the core parts below with typer instead of python's default parser.  It also just runs from a config.yaml (can obviously change the path to the weights here), but for my workflows that would normally point to the original weights that I started training from. Not my best checkpoint. So I added an additional argument to point to those trained weights.</p>"},{"location":"deployment/intro/#deployment","title":"Deployment","text":""},{"location":"deployment/intro/#options","title":"Options","text":"<ul> <li>Torchscript </li> <li>Gotchas </li> <li>Make sure to import torchscript before reloading the saved model </li> <li> <p>Show what would be hit, show successful reload. </p> </li> <li> <p>ONNX </p> </li> <li>Because it's a 'universal' framework, it offers thje most functionality wrt further optimizations (e.g. operator fusing or conversion to fp16 or int8) </li> </ul>"},{"location":"deployment/intro/#preprocessing","title":"Preprocessing","text":"<p>Within detectron2 preprocessing is managed by a predictor object, but if you're using torchscript or ONNX, you're trying to remove your  dependence on detectron2. In order to achieve this aim I simply extracted the preprocessing code from the predictor object.</p>"},{"location":"deployment/onnx/","title":"onnx","text":"<p>onnx</p>"},{"location":"deployment/torchscript/","title":"torchscript","text":"<p>torchscript</p>"}]}