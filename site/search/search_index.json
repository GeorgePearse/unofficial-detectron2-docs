{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#motivation","title":"Motivation","text":"<p>The detectron2 API is incredible, but the documentation and response to GitHub issues are incredibly bad. </p> <p>This is very wasteful, and felt like an easy thing to start fixing. </p> <p>Between a lot of different GitHub issues and Stack discussions, a lot of the problems contained here were already solved, but a lot of time had to be put into searching Google in order to find those, parse what you cared about and implement the solutions. This is meant to be a one stop shop.</p> <p>(Note I'd love to upstream this content to the original Detectron2 docs if they'd allow it, but for the timebeing this will do)</p>"},{"location":"#about-me","title":"About Me","text":"<p>2 years in Medical Imaging startup, now working in AI for Waste Management / Recycling. I work at Binit AI as their Founding MLE. All of my team are excellent, and I enjoy every day I get to work with them. </p> <p>Tutorials have been written with a lot of help from Jack Tattershall (https://www.linkedin.com/in/jack-tattershall-9bb972190). A very old friend who meandered his way to Computer Vision alongside me.</p>"},{"location":"deployment/intro/","title":"Intro","text":""},{"location":"deployment/intro/#how-to-deploy-models-trained-with-detectron2","title":"How to Deploy Models Trained with Detectron2","text":"<p>The repo comes with an unassuming script called export_model.py, it uses the rest of the package just as an API, and can be used as a standalone script or copied into your own repo (so that you don't have to clone detectron2).</p> <p>It is overly verbose, so I've rewritten the core parts below with typer instead of python's default parser.  It also just runs from a config.yaml (can obviously change the path to the weights here), but for my workflows that would normally point to the original weights that I started training from. Not my best checkpoint. So I added an additional argument to point to those trained weights.</p> <pre><code>import typer \n\ndef main(\n        export_format: str = 'torchscript',\n        architecture_name: str = 'R101',\n        checkpoint_path: str = None,\n    ): \n    DetectionCheckpointer()\n\nif __name__ == '__main__':\n    typer.run(main)\n</code></pre>"},{"location":"deployment/intro/#deployment","title":"Deployment","text":""},{"location":"deployment/intro/#options","title":"Options","text":"<ul> <li>Torchscript </li> <li>Gotchas </li> <li>Make sure to import torchscript before reloading the saved model </li> <li> <p>Show what would be hit, show successful reload. </p> </li> <li> <p>ONNX </p> </li> <li>Because it's a 'universal' framework, it offers thje most functionality wrt further optimizations (e.g. operator fusing or conversion to fp16 or int8) </li> </ul>"},{"location":"deployment/intro/#preprocessing","title":"Preprocessing","text":"<p>Within detectron2 preprocessing is managed by a predictor object, but if you're using torchscript or ONNX, you're trying to remove your  dependence on detectron2. In order to achieve this aim I simply extracted the preprocessing code from the predictor object.</p>"},{"location":"deployment/onnx/","title":"ONNX","text":"<p>onnx</p>"},{"location":"deployment/torchscript/","title":"Torchscript","text":"<p>torchscript</p>"},{"location":"experiment-tracking/aim/","title":"Aim","text":""},{"location":"experiment-tracking/aim/#experiment-tracking-with-aim","title":"Experiment Tracking with Aim","text":"<p>This was more or less a copy paste of the tracker written for MLFlow available here. </p> <p>https://philenius.github.io/machine%20learning/2022/01/09/how-to-log-artifacts-metrics-and-parameters-of-your-detectron2-model-training-to-mlflow.html</p> <p>The one caveat is that their implementation did not support multi-gpu training. That required a step to use 'comm' in order to check whether the process in which the code was being run was the 'main' process. Without this check an experiment was tracked  for every GPU being used to run training e.g. you run python train.py and get an experiment for each GPU.</p> <pre><code>from detectron2.engine import HookBase\nfrom aim import Run\nimport torch\nimport os\nimport detectron2.utils.comm as comm\nfrom datetime import datetime\n\nAIM_URL = os.environ[\"AIM_URL\"]\n\nclass AimHook(HookBase):\n    \"\"\"\n    A custom hook class that logs artifacts, metrics, and parameters to MLflow.\n\n    All taken from https://philenius.github.io/machine%20learning/2022/01/09/how-to-log-artifacts-metrics-and-parameters-of-your-detectron2-model-training-to-mlflow.html\n    And adapted for Aim\n\n    Looking at write_metrics in this file can help with further development.\n    https://github.com/facebookresearch/detectron2/blob/80e2673da161f57afe37ef769836a61976108ef1/detectron2/engine/train_loop.py#LL346\n    \"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg.clone()\n\n    def before_train(self):\n\n        clean_datetime = str(datetime.now()).replace(' ','_').replace(':','-')\n\n        # Have to check if it's the main process so that you dont \n        # get multiple tracked run for a single, multi-gpu process.s\n        if comm.is_main_process():\n            self.run = Run(\n                repo=AIM_URL,\n                experiment=clean_datetime,\n            )\n            self.run['hparams'] = self.cfg\n\n    def after_step(self):\n        # Only write metrics if it's the main process\n        if comm.is_main_process():\n            with torch.no_grad():\n                latest_metrics = self.trainer.storage.latest()\n                for k, v in latest_metrics.items():\n                    self.run.track(name=k, value=v[0], step=self.trainer.storage.iter)\n\n    def after_train(self):\n        with torch.no_grad():\n            with open(os.path.join(self.cfg.OUTPUT_DIR, \"model-config.yaml\"), \"w\") as f:\n                f.write(self.cfg.dump())\n</code></pre>"},{"location":"experiment-tracking/tensorboard/","title":"Tensorboard","text":""},{"location":"experiment-tracking/tensorboard/#experiment-tracking-with-tensorboard","title":"Experiment Tracking with Tensorboard","text":"<p>This comes out of the box when you train with Detectron2. I'd recommend setting the OUTPUT_DIR on the config, so that  each run goes into its own directory, greatly simplifying run comparisons.</p>"}]}