{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#motivation","title":"Motivation","text":"<p>The detectron2 API is incredible, but the documentation and response to GitHub issues are incredibly bad. </p> <p>This is very wasteful, and felt like an easy thing to start fixing. </p> <p>Between a lot of different GitHub issues and Stack discussions, a lot of the problems contained here were already solved, but a lot of time had to be put into searching Google in order to find those, parse what you cared about and implement the solutions. This is meant to be a one stop shop.</p> <p>(Note I'd love to upstream this content to the original Detectron2 docs if they'd allow it, but for the timebeing this will do)</p>"},{"location":"#about-me","title":"About Me","text":"<p>2 years in Medical Imaging startup, now working in AI for Waste Management / Recycling. I work at Binit AI as their Founding MLE. All of my team are excellent, and I enjoy every day I get to work with them. </p> <p>Tutorials have been written with a lot of help from Jack Tattershall (https://www.linkedin.com/in/jack-tattershall-9bb972190). A very old friend who meandered his way to Computer Vision alongside me.</p>"},{"location":"advanced-projects/projects/","title":"Advanced Projects","text":"<p>A few advanced research projects are built on top of Detectron2, it can be valuable to read through their code to get ideas and implementations.</p> <p>The most significant are listed here https://github.com/facebookresearch/detectron2/tree/main/projects. </p>"},{"location":"benchmarks/benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/benchmarks/#inference-speed-benchmarks-cpu","title":"Inference Speed Benchmarks (CPU)","text":""},{"location":"benchmarks/benchmarks/#inference-speed-benchmarks-gpu","title":"Inference Speed Benchmarks (GPU)","text":""},{"location":"benchmarks/benchmarks/#coco-metrics","title":"COCO Metrics","text":""},{"location":"checkpointing/checkpointing/","title":"Checkpointing","text":""},{"location":"checkpointing/checkpointing/#google-cloud-storage","title":"Google Cloud Storage","text":""},{"location":"checkpointing/checkpointing/#aws","title":"AWS","text":""},{"location":"config/intro/","title":"Configurable Parameters","text":""},{"location":"config/intro/#configurable-parameters-in-detectron2","title":"Configurable Parameters in Detectron2","text":"<p>Detectron2 is a config heavy API, really, this is one of its main strength. Train a diverse set of models without needing to edit any actual Python code.</p> <p>That said there's an overwhelming number of options, it's hard to find out what many of them do, and it's often difficult to find what you're looking for.</p>"},{"location":"config/intro/#model","title":"Model","text":"<pre><code>cfg.MODEL.BACKBONE.FREEZE_AT = 2\n</code></pre> <p>How much of your model you want to be frozen.</p> <pre><code>cfg.MODEL.BACKBONE.FREEZE_AT = 0 \n</code></pre> <p>Would unfreeze your whole model. </p>"},{"location":"config/intro/#solver","title":"Solver","text":""},{"location":"config/intro/#how-to-drop-the-learning-rate-at-set-epochs","title":"How to Drop the Learning Rate at set epochs","text":""},{"location":"deployment/intro/","title":"Intro","text":""},{"location":"deployment/intro/#how-to-deploy-models-trained-with-detectron2","title":"How to Deploy Models Trained with Detectron2","text":"<p>The repo comes with an unassuming script called export_model.py, it uses the rest of the package just as an API, and can be used as a standalone script or copied into your own repo (so that you don't have to clone detectron2).</p> <p>It is overly verbose, so I've rewritten the core parts below with typer instead of python's default parser.  It also just runs from a config.yaml (can obviously change the path to the weights here), but for my workflows that would normally point to the original weights that I started training from. Not my best checkpoint. So I added an additional argument to point to those trained weights.</p> <pre><code>import typer \n\ndef main(\n        export_format: str = 'torchscript',\n        architecture_name: str = 'R101',\n        checkpoint_path: str = None,\n    ): \n    DetectionCheckpointer()\n\nif __name__ == '__main__':\n    typer.run(main)\n</code></pre>"},{"location":"deployment/intro/#deployment","title":"Deployment","text":""},{"location":"deployment/intro/#options","title":"Options","text":"<ul> <li>Torchscript </li> <li>Gotchas </li> <li>Make sure to import torchscript before reloading the saved model </li> <li> <p>Show what would be hit, show successful reload. </p> </li> <li> <p>ONNX </p> </li> <li>Because it's a 'universal' framework, it offers thje most functionality wrt further optimizations (e.g. operator fusing or conversion to fp16 or int8) </li> </ul>"},{"location":"deployment/intro/#preprocessing","title":"Preprocessing","text":"<p>Within detectron2 preprocessing is managed by a predictor object, but if you're using torchscript or ONNX, you're trying to remove your  dependence on detectron2. In order to achieve this aim I simply extracted the preprocessing code from the predictor object.</p>"},{"location":"deployment/onnx/","title":"ONNX","text":""},{"location":"deployment/onnx/#exporting-a-model-to-onnx","title":"Exporting a Model to ONNX","text":"<p>For the timebeing don't, even once the model is in ONNX format (this export runs smoothly), some are only supported by the Caffe2 backend. This can be a pain to install,  so really you're losing most of the point of converting to ONNX in the first place. </p> <p>(This is obviously just my opinion)</p>"},{"location":"deployment/torchscript/","title":"Torchscript","text":""},{"location":"deployment/torchscript/#exporting-a-model-to-torchscript","title":"Exporting a model to Torchscript","text":"<pre><code>import torch\n\n#\u00a0Note, this is needed, despite not being explicitly called\nimport torchvision \n\nmodel = torch.jit.load('models/model.ts')\n</code></pre>"},{"location":"distributed-training/distributed-training/","title":"Distributed Training","text":""},{"location":"distributed-training/distributed-training/#distributed-training","title":"Distributed Training","text":"<p>Detectron2 only supports DDP (Distributed Data Parralel)</p> <pre><code>launch(\n    run_training,\n    num_gpus,\n    num_machines=1,\n    machine_rank=0,\n    dist_url='auto',\n    args=(cfg,),\n)\n</code></pre>"},{"location":"experiment-tracking/aim/","title":"Aim","text":""},{"location":"experiment-tracking/aim/#experiment-tracking-with-aim","title":"Experiment Tracking with Aim","text":"<p>This was more or less a copy paste of the tracker written for MLFlow available here. </p> <p>https://philenius.github.io/machine%20learning/2022/01/09/how-to-log-artifacts-metrics-and-parameters-of-your-detectron2-model-training-to-mlflow.html</p> <p>The one caveat is that their implementation did not support multi-gpu training. That required a step to use 'comm' in order to check whether the process in which the code was being run was the 'main' process. Without this check an experiment was tracked  for every GPU being used to run training e.g. you run python train.py and get an experiment for each GPU.</p> <pre><code>from detectron2.engine import HookBase\nfrom aim import Run\nimport torch\nimport os\nimport detectron2.utils.comm as comm\nfrom datetime import datetime\n\nAIM_URL = os.environ[\"AIM_URL\"]\n\nclass AimHook(HookBase):\n    \"\"\"\n    A custom hook class that logs artifacts, metrics, and parameters to MLflow.\n\n    All taken from https://philenius.github.io/machine%20learning/2022/01/09/how-to-log-artifacts-metrics-and-parameters-of-your-detectron2-model-training-to-mlflow.html\n    And adapted for Aim\n\n    Looking at write_metrics in this file can help with further development.\n    https://github.com/facebookresearch/detectron2/blob/80e2673da161f57afe37ef769836a61976108ef1/detectron2/engine/train_loop.py#LL346\n    \"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg.clone()\n\n    def before_train(self):\n\n        clean_datetime = str(datetime.now()).replace(' ','_').replace(':','-')\n\n        # Have to check if it's the main process so that you dont \n        # get multiple tracked run for a single, multi-gpu process.s\n        if comm.is_main_process():\n            self.run = Run(\n                repo=AIM_URL,\n                experiment=clean_datetime,\n            )\n            self.run['hparams'] = self.cfg\n\n    def after_step(self):\n        # Only write metrics if it's the main process\n        if comm.is_main_process():\n            with torch.no_grad():\n                latest_metrics = self.trainer.storage.latest()\n                for k, v in latest_metrics.items():\n                    self.run.track(name=k, value=v[0], step=self.trainer.storage.iter)\n\n    def after_train(self):\n        if comm.is_main_process():\n            with torch.no_grad():\n                with open(os.path.join(self.cfg.OUTPUT_DIR, \"model-config.yaml\"), \"w\") as f:\n                    f.write(self.cfg.dump())\n</code></pre> <p>You then need to register your hook to the trainer, this looks something like </p> <pre><code>aim_hook = AimHook(cfg)\ntrainer = DefaultTrainer()\ntrainer.register_hooks(hooks=[aim_hook])\n</code></pre>"},{"location":"experiment-tracking/mlflow/","title":"MLFlow","text":""},{"location":"experiment-tracking/mlflow/#experiment-tracking-with-mlflow","title":"Experiment Tracking with MLFlow","text":"<pre><code>from detectron2.engine import HookBase\nfrom aim import Run\nimport torch\nimport os\nimport detectron2.utils.comm as comm\nfrom datetime import datetime\n\nMLFLow_URL = os.environ[\"MLFlow_URL\"]\n\nclass MLFlowHook(HookBase):\n    \"\"\"\n    A custom hook class that logs artifacts, metrics, and parameters to MLflow.\n\n    All taken from https://philenius.github.io/machine%20learning/2022/01/09/how-to-log-artifacts-metrics-and-parameters-of-your-detectron2-model-training-to-mlflow.html\n    And adapted for Aim\n\n    Looking at write_metrics in this file can help with further development.\n    https://github.com/facebookresearch/detectron2/blob/80e2673da161f57afe37ef769836a61976108ef1/detectron2/engine/train_loop.py#LL346\n    \"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg.clone()\n\n    def before_train(self):\n\n        clean_datetime = str(datetime.now()).replace(' ','_').replace(':','-')\n\n        # Have to check if it's the main process so that you dont \n        # get multiple tracked run for a single, multi-gpu process.s\n        if comm.is_main_process():\n            self.run = Run(\n                repo=AIM_URL,\n                experiment=clean_datetime,\n            )\n            self.run['hparams'] = self.cfg\n\n    def after_step(self):\n        # Only write metrics if it's the main process\n        if comm.is_main_process():\n            with torch.no_grad():\n                latest_metrics = self.trainer.storage.latest()\n                for k, v in latest_metrics.items():\n                    self.run.track(name=k, value=v[0], step=self.trainer.storage.iter)\n\n    def after_train(self):\n        with torch.no_grad():\n            with open(os.path.join(self.cfg.OUTPUT_DIR, \"model-config.yaml\"), \"w\") as f:\n                f.write(self.cfg.dump())\n</code></pre> <pre><code>from detectron2.engine import HookBase\nimport detectron2.utils.comm as comm\nimport mlflow\n\n\nclass MLflowHook(HookBase):\n    \"\"\"\n    A custom hook class that logs artifacts, metrics, and parameters to MLflow.\n    \"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg.clone()\n\n    def before_train(self):\n        if comm.is_main_process():\n            with torch.no_grad():\n                mlflow.set_tracking_uri(self.cfg.MLFLOW.TRACKING_URI)\n                mlflow.set_experiment(self.cfg.MLFLOW.EXPERIMENT_NAME)\n                mlflow.start_run(run_name=self.cfg.MLFLOW.RUN_NAME)\n                mlflow.set_tag(\"mlflow.note.content\",\n                               self.cfg.MLFLOW.RUN_DESCRIPTION)\n                for k, v in self.cfg.items():\n                    mlflow.log_param(k, v)\n\n    def after_step(self):\n        if comm.is_main_process():\n            with torch.no_grad():\n                latest_metrics = self.trainer.storage.latest()\n                for k, v in latest_metrics.items():\n                    mlflow.log_metric(key=k, value=v[0], step=v[1])\n\n    def after_train(self):\n        if comm.is_main_process():\n            with torch.no_grad():\n                with open(os.path.join(self.cfg.OUTPUT_DIR, \"model-config.yaml\"), \"w\") as f:\n                    f.write(self.cfg.dump())\n                mlflow.log_artifacts(self.cfg.OUTPUT_DIR)\n</code></pre> <p>You then need to register the hook with your trainer</p> <pre><code>mlflow_hook = MLFlowHook(cfg)\ntrainer = DefaultTrainer()\ntrainer.register_hooks(hooks=[mlflow_hook])\n</code></pre>"},{"location":"experiment-tracking/tensorboard/","title":"Tensorboard","text":""},{"location":"experiment-tracking/tensorboard/#experiment-tracking-with-tensorboard","title":"Experiment Tracking with Tensorboard","text":"<p>This comes out of the box when you train with Detectron2. I'd recommend setting the OUTPUT_DIR on the config, so that  each run goes into its own directory, greatly simplifying run comparisons.</p> <pre><code>clean_datetime = str(datetime.now()).replace(' ','_').replace(':','-')\ncfg.OUTPUT_DIR = f'output/{clean_datetime}'\n</code></pre> <p>Limitations of Tensorboard:  * Can't add notes to runs without further plugins.</p>"}]}